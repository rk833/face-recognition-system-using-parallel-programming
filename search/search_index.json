{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Parallel Face Recognition System","text":"<p>This project implements a high-performance face recognition system using both serial and parallel processing techniques. The main objective is to demonstrate how parallel computing can significantly improve execution time when performing face matching on large image datasets.</p> <p>The system uses the <code>face_recognition</code> library for facial encoding and comparison, and applies multiprocessing with dynamic worker allocation to distribute the workload efficiently across available CPU cores.</p> <p>Key features of this project include:</p> <ul> <li>Serial face recognition for baseline performance comparison  </li> <li>Parallel face recognition using multiprocessing  </li> <li>Intelligent dynamic worker allocation based on dataset size  </li> <li>Automatic chunk size optimisation  </li> <li>Load balancing using a work-stealing strategy  </li> <li>Detailed performance statistics and speedup analysis  </li> </ul> <p>The documentation provides:</p> <ul> <li>An overview of the system design  </li> <li>Explanation of serial and parallel implementations  </li> <li>API references for all major modules  </li> <li>Performance and scalability behaviour  </li> </ul> <p>This project was developed as part of an Advanced Algorithms assessment to explore data parallelism, scalability, and real-world performance optimisation.</p>"},{"location":"main/","title":"Main Program","text":"<p>This module acts as the control centre of the system. It manages user interaction, runs serial and parallel recognition, and performs performance evaluation.</p>"},{"location":"main/#responsibilities","title":"Responsibilities","text":"<ul> <li>Display system features and configuration  </li> <li>Run serial face recognition for benchmarking  </li> <li>Launch parallel recognition with dynamic workers  </li> <li>Measure execution time and throughput  </li> <li>Calculate speedup and parallel efficiency  </li> <li>Print detailed performance reports  </li> </ul>"},{"location":"main/#api-reference","title":"API Reference","text":""},{"location":"main/#task_1_4_main","title":"task_1_4_main","text":"<p>parallel face recognition system (task 1.4) - enhanced version</p> <p>features intelligent dynamic worker allocation based on dataset size. automatically optimizes worker count and chunk size for maximum efficiency.</p>"},{"location":"main/#task_1_4_main.demonstrate_scalability","title":"demonstrate_scalability","text":"<pre><code>demonstrate_scalability()\n</code></pre> <p>demonstrate how worker allocation scales with dataset size.</p> Source code in <code>task_1_4_main.py</code> <pre><code>def demonstrate_scalability():\n    \"\"\"\n    demonstrate how worker allocation scales with dataset size.\n    \"\"\"\n    print_header(\"Dynamic worker allocation demonstration\")\n    print(\"\\nThis shows how the system adapts to different dataset sizes:\\n\")\n\n    from multiprocessing import cpu_count\n    cores = cpu_count()\n\n    # simulate different dataset sizes\n    scenarios = [\n        (5, \"very small dataset\"),\n        (25, \"small dataset\"),\n        (100, \"medium dataset\"),\n        (500, \"large dataset\"),\n        (2000, \"very large dataset\"),\n    ]\n\n    print(f\"{'dataset size':&lt;20} {'workers':&lt;12} {'chunk size':&lt;15} {'strategy'}\")\n    print(\"-\" * 70)\n\n    for size, description in scenarios:\n        # simulate the logic from _determine_optimal_processes\n        if size &lt;= 10:\n            workers = min(max(1, size // 3), 4)\n            chunk = 1\n            strategy = \"overhead min\"\n        elif size &lt;= 50:\n            workers = min(size // 4, cores // 2)\n            workers = max(workers, 2)\n            chunk = max(1, size // (workers * 3))\n            strategy = \"proportional\"\n        elif size &lt;= 200:\n            workers = min(size // 3, int(cores * 0.75))\n            workers = max(workers, 4)\n            chunk = 2\n            strategy = \"balanced\"\n        elif size &lt;= 1000:\n            workers = min(size // 5, cores)\n            chunk = max(2, size // (workers * 4))\n            strategy = \"high parallel\"\n        else:\n            workers = cores\n            chunk = max(3, size // (workers * 10))\n            strategy = \"full parallel\"\n\n        workers = min(workers, size)\n\n        print(f\"{size:&gt;4} images ({description:&lt;12}) {workers:&gt;2} workers    \"\n              f\"chunk={chunk:&lt;10} {strategy}\")\n</code></pre>"},{"location":"main/#task_1_4_main.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>main execution function with intelligent dynamic worker allocation.</p> Source code in <code>task_1_4_main.py</code> <pre><code>def main():\n    \"\"\"\n    main execution function with intelligent dynamic worker allocation.\n    \"\"\"\n\n    print_header(\"Parallel face recognition system\")\n    print(\"\\nfeatures:\")\n    print(\"  dynamic worker allocation based on dataset size\")\n    print(\"  automatic chunk size optimization\")\n    print(\"  overhead-aware scaling strategies\")\n    print(\"  load balancing with work stealing\")\n    print(\"  performance profiling and comparison\")\n\n    # show scalability demonstration\n    demonstrate_scalability()\n\n    # configuration\n    base_dir = os.path.dirname(os.path.abspath(__file__))\n    known_image_path = os.path.join(base_dir, \"known_man.jpg\")\n    image_folder_path = os.path.join(base_dir, \"imageset\")\n    output_folder_path = os.path.join(base_dir, \"faces_detected\")\n\n    # validate paths\n    if not os.path.exists(known_image_path):\n        print(f\"\\nerror: known face image not found at: {known_image_path}\")\n        return\n\n    if not os.path.exists(image_folder_path):\n        print(f\"\\nerror: image folder not found at: {image_folder_path}\")\n        return\n\n    # user choice: serial or parallel comparison\n    print_section(\"performance comparison mode\")\n    print(\"\\ndo you want to run serial comparison?\")\n    print(\"  y = yes, compare with serial version (recommended for datasets &lt; 100)\")\n    print(\"  n = no, run optimized parallel only\")\n\n    choice = input(\"\\nyour choice [y/n]: \").strip().lower()\n\n    serial_time = None\n    if choice == 'y':\n        try:\n            serial_time = run_serial_comparison(\n                known_image_path, image_folder_path, output_folder_path\n            )\n        except Exception as e:\n            print(f\"serial comparison failed: {e}\")\n            print(\"  continuing with parallel processing...\")\n\n    # parallel processing with dynamic workers\n    print_header(\"Parallel processing\")\n\n    try:\n        # initialize recognizer\n        recognizer = FaceRecognizer(\n            known_image_path, \n            image_folder_path, \n            output_folder_path\n        )\n\n        # run parallel recognition with dynamic worker allocation\n        parallel_start = time.time()\n        matched_files, total_images = recognizer.run_parallel_recognition()\n        parallel_time = time.time() - parallel_start\n\n        # results summary\n        print_header(\"processing complete\")\n\n        print(f\"\\nprocessing statistics:\")\n        print(f\"  total images processed:  {total_images}\")\n        print(f\"  matches found:           {len(matched_files)}\")\n        if total_images &gt; 0:\n            print(f\"  match rate:              {len(matched_files)/total_images*100:.1f}%\")\n\n        if total_images &gt; 0:\n            print(f\"\\ntiming analysis:\")\n            print(f\"  parallel time:           {parallel_time:.3f}s\")\n            print(f\"  avg per image:           {parallel_time/total_images:.3f}s\")\n            print(f\"  throughput:              {total_images/parallel_time:.2f} images/sec\")\n\n        # performance comparison\n        if serial_time:\n            speedup = serial_time / parallel_time\n            efficiency = (speedup / os.cpu_count()) * 100\n\n            print(f\"\\nspeedup analysis:\")\n            print(f\"  serial time:             {serial_time:.3f}s\")\n            print(f\"  parallel time:           {parallel_time:.3f}s\")\n            print(f\"  speedup:                 {speedup:.2f}x faster\")\n            print(f\"  parallel efficiency:     {efficiency:.1f}%\")\n            print(f\"  time saved:              {serial_time - parallel_time:.3f}s\")\n\n            # efficiency rating\n            if efficiency &gt;= 80:\n                rating = \"excellent\"\n            elif efficiency &gt;= 60:\n                rating = \"good\"\n            elif efficiency &gt;= 40:\n                rating = \"fair\"\n            else:\n                rating = \"poor - consider optimization\"\n            print(f\"  efficiency rating:       {rating}\")\n\n        # performance breakdown\n        print(recognizer.get_performance_report())\n\n        # matched files list\n        if matched_files:\n            print(f\"\\nmatched files ({len(matched_files)}):\")\n            for i, filename in enumerate(matched_files, 1):\n                print(f\"  {i:2d}. {filename}\")\n        else:\n            print(\"\\nno matches found in the dataset.\")\n\n        # system info\n        print(f\"\\nsystem information:\")\n        print(f\"  cpu cores available:     {os.cpu_count()}\")\n        print(f\"  cpu cores used:          {recognizer.performance_stats.get('num_workers', 'n/a')}\")\n        print(f\"  utilization:             {(recognizer.performance_stats.get('num_workers', 0) / os.cpu_count() * 100):.1f}%\")\n        print(f\"  parallelization model:   data decomposition (mimd)\")\n        print(f\"  memory model:            distributed memory\")\n        print(f\"  load balancing:          dynamic (work stealing)\")\n        print(f\"  chunk size:              {recognizer.performance_stats.get('chunksize', 'n/a')}\")\n\n        print(\"All processing complete!\".center(70))\n\n    except Exception as e:\n        print(f\"\\nfatal error: {e}\")\n        import traceback\n        traceback.print_exc()\n        return\n</code></pre>"},{"location":"main/#task_1_4_main.print_header","title":"print_header","text":"<pre><code>print_header(title)\n</code></pre> <p>print formatted section header</p> Source code in <code>task_1_4_main.py</code> <pre><code>def print_header(title):\n    \"\"\"print formatted section header\"\"\"\n    print(\"\\n\" + \"-\"*70)\n    print(title.center(70))\n    print(\"-\"*70)\n</code></pre>"},{"location":"main/#task_1_4_main.print_section","title":"print_section","text":"<pre><code>print_section(title)\n</code></pre> <p>print formatted subsection</p> Source code in <code>task_1_4_main.py</code> <pre><code>def print_section(title):\n    \"\"\"print formatted subsection\"\"\"\n    print(f\"\\n{title}\")\n</code></pre>"},{"location":"main/#task_1_4_main.run_serial_comparison","title":"run_serial_comparison","text":"<pre><code>run_serial_comparison(known_image_path, image_folder_path, output_folder_path)\n</code></pre> <p>run serial version for performance comparison.</p> <p>Returns:</p> Type Description <p>serial execution time in seconds</p> Source code in <code>task_1_4_main.py</code> <pre><code>def run_serial_comparison(known_image_path, image_folder_path, \n                         output_folder_path):\n    \"\"\"\n    run serial version for performance comparison.\n\n    returns:\n        serial execution time in seconds\n    \"\"\"\n    import face_recognition\n\n    print_section(\"serial processing (for comparison)\")\n\n    start_time = time.time()\n\n    # load known face\n    print(\"loading known face...\")\n    known_image = face_recognition.load_image_file(known_image_path)\n    known_encoding = face_recognition.face_encodings(known_image)[0]\n\n    # get all images\n    print(\"scanning images...\")\n    filenames = [f.name for f in os.scandir(image_folder_path) if f.is_file()]\n    total_images = len(filenames)\n\n    print(f\"processing {total_images} images serially (one at a time)...\")\n    matches = 0\n\n    # process each image one by one (serial)\n    for i, filename in enumerate(filenames, 1):\n        try:\n            image_path = os.path.join(image_folder_path, filename)\n            unknown_image = face_recognition.load_image_file(image_path)\n            unknown_encodings = face_recognition.face_encodings(unknown_image)\n\n            for unknown_encoding in unknown_encodings:\n                result = face_recognition.compare_faces(\n                    [known_encoding], unknown_encoding, tolerance=0.6\n                )\n                if result[0]:\n                    matches += 1\n                    print(f\"  [{i}/{total_images}] match: {filename}\")\n                    break\n        except:\n            pass\n\n    serial_time = time.time() - start_time\n\n    print(f\"\\nserial processing complete\")\n    print(f\"  total time: {serial_time:.2f}s\")\n    print(f\"  matches found: {matches}\")\n\n    return serial_time\n</code></pre>"},{"location":"recognizer/","title":"Parallel Recognizer","text":"<p>This module implements the core parallel face recognition engine. It distributes images across multiple processes and dynamically adjusts worker allocation to maximise CPU utilisation.</p>"},{"location":"recognizer/#features","title":"Features","text":"<ul> <li>Dynamic worker selection  </li> <li>Adaptive chunk sizing  </li> <li>Load balancing using work-stealing  </li> <li>Parallel encoding and matching  </li> <li>Internal performance monitoring  </li> </ul>"},{"location":"recognizer/#api-reference","title":"API Reference","text":""},{"location":"recognizer/#src.recognizer","title":"recognizer","text":""},{"location":"recognizer/#src.recognizer.FaceRecognizer","title":"FaceRecognizer","text":"<p>Handles parallel face recognition using multiprocessing with worker allocation.</p> Source code in <code>src\\recognizer.py</code> <pre><code>class FaceRecognizer:\n    \"\"\"\n    Handles parallel face recognition using multiprocessing with worker allocation.\n    \"\"\"\n    def __init__(self, known_path, image_folder, output_folder):\n        \"\"\"\n        Initialize paths and performance tracking.\n        \"\"\"\n        self.known_path = known_path\n        self.image_folder = image_folder\n        self.output_folder = output_folder\n        self.known_encoding = None\n        self.performance_stats = {\n            'load_time': 0,\n            'scan_time': 0,\n            'processing_time': 0,\n            'total_time': 0,\n            'worker_allocation_strategy': ''\n        }\n\n    def _load_known_face(self):\n        \"\"\"\n        Loads the known face image and extracts its encoding.\n\n        Raises:\n            FileNotFoundError: If known face image doesn't exist\n            ValueError: If no face detected in known image\n        \"\"\"\n        print(\"\\nloading known face...\")\n        load_start = time.time()\n\n        if not os.path.exists(self.known_path):\n            raise FileNotFoundError(f\"known face image not found: {self.known_path}\")\n\n        known_image = face_recognition.load_image_file(self.known_path)\n        encodings = face_recognition.face_encodings(known_image)\n\n        if not encodings:\n            raise ValueError(f\"no face detected in: {self.known_path}\")\n\n        self.known_encoding = encodings[0]\n        self.performance_stats['load_time'] = time.time() - load_start\n        print(f\"  known face loaded ({self.performance_stats['load_time']:.3f}s)\")\n\n    def _get_image_files(self):\n        \"\"\"\n        Return a list of valid image files from the image folder.\n        \"\"\"\n        print(\"\\nscanning image folder...\")\n        scan_start = time.time()\n\n        if not os.path.exists(self.image_folder):\n            raise FileNotFoundError(f\"image folder not found: {self.image_folder}\")\n\n        # get all files (filters out directories)\n        filenames = [f.name for f in os.scandir(self.image_folder) if f.is_file()]\n\n        # filter for image extensions only\n        valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif'}\n        filenames = [f for f in filenames \n                    if os.path.splitext(f.lower())[1] in valid_extensions]\n\n        self.performance_stats['scan_time'] = time.time() - scan_start\n        print(f\"  found {len(filenames)} images ({self.performance_stats['scan_time']:.3f}s)\")\n\n        return filenames\n\n    def _determine_optimal_processes(self, num_images):\n        \"\"\"\n        Determine optimal worker count and chunk size based on dataset size.\n\n        Returns:\n            Tuple[int, int]: (num_processes, chunksize)\n        \"\"\"\n        available_cores = cpu_count()\n\n        # very small dataset (1-10 images)\n        if num_images &lt;= 10:\n            # overhead of creating processes outweighs benefits\n            optimal = min(max(1, num_images // 3), 4)\n            chunksize = 1\n            strategy = \"very small dataset strategy (overhead minimization)\"\n            print(f\"strategy: very small dataset\")\n            print(f\"   using {optimal} workers (1 per ~3 images to minimize overhead)\")\n            print(f\"   chunk size: {chunksize}\")\n\n        # small dataset (11-50 images)\n        elif num_images &lt;= 50:\n            optimal = min(num_images // 4, available_cores // 2)\n            optimal = max(optimal, 2)\n            chunksize = max(1, num_images // (optimal * 3))\n            strategy = \"small dataset strategy (proportional scaling)\"\n            print(f\"strategy: small dataset\")\n            print(f\"   using {optimal} workers (~1 per 4 images)\")\n            print(f\"   chunk size: {chunksize}\")\n\n        # medium dataset (51-200 images)\n        elif num_images &lt;= 200:\n            optimal = min(num_images // 3, int(available_cores * 0.75))\n            optimal = max(optimal, 4)\n            chunksize = 2\n            strategy = \"medium dataset strategy (balanced utilization)\"\n            print(f\"strategy: medium dataset\")\n            print(f\"   using {optimal} workers (~75% of cores)\")\n            print(f\"   chunk size: {chunksize}\")\n\n        # large dataset (201-1000 images)\n        elif num_images &lt;= 1000:\n            optimal = min(num_images // 5, available_cores)\n            chunksize = max(2, num_images // (optimal * 4))\n            strategy = \"large dataset strategy (high parallelism)\"\n            print(f\"strategy: large dataset\")\n            print(f\"   using {optimal} workers (maximizing core usage)\")\n            print(f\"   chunk size: {chunksize}\")\n\n        # very large dataset (1000+ images)\n        else:\n            optimal = available_cores\n            chunksize = max(3, num_images // (optimal * 10))\n            strategy = \"very large dataset strategy (full parallelism)\"\n            print(f\"strategy: very large dataset\")\n            print(f\"   using all {optimal} cores\")\n            print(f\"   chunk size: {chunksize} (optimized for throughput)\")\n\n        # don't create more workers than images\n        optimal = min(optimal, num_images)\n\n        # store strategy for reporting\n        self.performance_stats['worker_allocation_strategy'] = strategy\n\n        return optimal, chunksize\n\n    def _calculate_workload_distribution(self, num_images, num_processes, \n                                         chunksize):\n        \"\"\"\n        Calculate and display how work will be distributed across workers.\n        \"\"\"\n        print(f\"\\nworkload distribution analysis:\")\n        print(f\"   total images:     {num_images}\")\n        print(f\"   worker processes: {num_processes}\")\n        print(f\"   chunk size:       {chunksize}\")\n\n        # calculate theoretical distribution\n        total_chunks = (num_images + chunksize - 1) // chunksize\n        chunks_per_worker_avg = total_chunks / num_processes\n\n        print(f\"   total chunks:     {total_chunks}\")\n        print(f\"   avg chunks/worker: {chunks_per_worker_avg:.2f}\")\n\n        # estimate based on dynamic scheduling\n        images_per_worker_base = num_images // num_processes\n        remainder = num_images % num_processes\n\n        if remainder &gt; 0:\n            print(f\"\\ndynamic distribution (estimated):\")\n            print(f\"   {num_processes - remainder} workers: ~{images_per_worker_base} images\")\n            print(f\"   {remainder} workers: ~{images_per_worker_base + 1} images\")\n        else:\n            print(f\"\\nbalanced distribution:\")\n            print(f\"   each worker: ~{images_per_worker_base} images\")\n\n        # calculate efficiency metrics\n        overhead_ratio = (num_processes * 0.05) / num_images\n        print(f\"\\nestimated process overhead: {overhead_ratio*100:.2f}%\")\n\n        if overhead_ratio &gt; 0.1:\n            print(f\"   warning: high overhead ratio - consider fewer workers\")\n        else:\n            print(f\"   good overhead/work ratio\")\n\n    def run_parallel_recognition(self):\n        \"\"\"\n        Run face recognition on all images using parallel processing.\n        \"\"\"\n        total_start = time.time()\n\n        # load known face\n        self._load_known_face()\n\n        # get all image files\n        filenames = self._get_image_files()\n        total_images = len(filenames)\n\n        if total_images == 0:\n            print(\"no images found to process\")\n            return [], 0\n\n        # determine optimal configuration\n        print(\"\\nconfiguring worker allocation...\")\n        print(f\"   available cpu cores: {cpu_count()}\")\n        print(f\"   dataset size: {total_images} images\")\n        print()\n\n        num_processes, chunksize = self._determine_optimal_processes(total_images)\n\n        # show workload distribution\n        self._calculate_workload_distribution(total_images, num_processes, chunksize)\n\n        # create worker function with fixed parameters\n        process_func = partial(\n            _worker_process_image,\n            known_encoding=self.known_encoding,\n            folder_path=self.image_folder,\n            output_folder=self.output_folder\n        )\n\n        # execute parallel processing\n        print(f\"\\nprocessing {total_images} images with {num_processes} workers...\")\n        print(f\"   (dynamic load balancing with chunk size: {chunksize})\")\n\n        processing_start = time.time()\n\n        with Pool(processes=num_processes) as pool:\n            results = pool.map(process_func, filenames, chunksize=chunksize)\n\n        self.performance_stats['processing_time'] = time.time() - processing_start\n\n        # filter and collect results\n        print(\"\\ncollecting results...\")\n\n        matched_results = [r for r in results if r is not None]\n        matched_files = [filename for filename, _ in matched_results]\n\n        if matched_results:\n            processing_times = [proc_time for _, proc_time in matched_results]\n            avg_match_time = sum(processing_times) / len(processing_times)\n            print(f\"   average time per matched image: {avg_match_time:.3f}s\")\n\n        self.performance_stats['total_time'] = time.time() - total_start\n        self.performance_stats['num_workers'] = num_processes\n        self.performance_stats['chunksize'] = chunksize\n\n        return matched_files, total_images\n\n    def get_performance_report(self):\n        \"\"\"\n        Return a formatted performance summary.\n        \"\"\"\n        report = \"\\n\" + \"-\"*70 + \"\\n\"\n        report += \"performance breakdown\\n\"\n        report += \"-\"*70 + \"\\n\"\n        report += f\"known face loading:    {self.performance_stats['load_time']:.3f}s\\n\"\n        report += f\"image folder scanning: {self.performance_stats['scan_time']:.3f}s\\n\"\n        report += f\"parallel processing:   {self.performance_stats['processing_time']:.3f}s\\n\"\n        report += f\"total execution time:  {self.performance_stats['total_time']:.3f}s\\n\\n\"\n\n        report += \"worker configuration\\n\"\n        report += \"-\"*70 + \"\\n\"\n        report += f\"strategy: {self.performance_stats.get('worker_allocation_strategy', 'n/a')}\\n\"\n        report += f\"workers used: {self.performance_stats.get('num_workers', 'n/a')}\\n\"\n        report += f\"chunk size: {self.performance_stats.get('chunksize', 'n/a')}\\n\"\n\n        return report\n</code></pre>"},{"location":"recognizer/#src.recognizer.FaceRecognizer.__init__","title":"__init__","text":"<pre><code>__init__(known_path, image_folder, output_folder)\n</code></pre> <p>Initialize paths and performance tracking.</p> Source code in <code>src\\recognizer.py</code> <pre><code>def __init__(self, known_path, image_folder, output_folder):\n    \"\"\"\n    Initialize paths and performance tracking.\n    \"\"\"\n    self.known_path = known_path\n    self.image_folder = image_folder\n    self.output_folder = output_folder\n    self.known_encoding = None\n    self.performance_stats = {\n        'load_time': 0,\n        'scan_time': 0,\n        'processing_time': 0,\n        'total_time': 0,\n        'worker_allocation_strategy': ''\n    }\n</code></pre>"},{"location":"recognizer/#src.recognizer.FaceRecognizer.get_performance_report","title":"get_performance_report","text":"<pre><code>get_performance_report()\n</code></pre> <p>Return a formatted performance summary.</p> Source code in <code>src\\recognizer.py</code> <pre><code>def get_performance_report(self):\n    \"\"\"\n    Return a formatted performance summary.\n    \"\"\"\n    report = \"\\n\" + \"-\"*70 + \"\\n\"\n    report += \"performance breakdown\\n\"\n    report += \"-\"*70 + \"\\n\"\n    report += f\"known face loading:    {self.performance_stats['load_time']:.3f}s\\n\"\n    report += f\"image folder scanning: {self.performance_stats['scan_time']:.3f}s\\n\"\n    report += f\"parallel processing:   {self.performance_stats['processing_time']:.3f}s\\n\"\n    report += f\"total execution time:  {self.performance_stats['total_time']:.3f}s\\n\\n\"\n\n    report += \"worker configuration\\n\"\n    report += \"-\"*70 + \"\\n\"\n    report += f\"strategy: {self.performance_stats.get('worker_allocation_strategy', 'n/a')}\\n\"\n    report += f\"workers used: {self.performance_stats.get('num_workers', 'n/a')}\\n\"\n    report += f\"chunk size: {self.performance_stats.get('chunksize', 'n/a')}\\n\"\n\n    return report\n</code></pre>"},{"location":"recognizer/#src.recognizer.FaceRecognizer.run_parallel_recognition","title":"run_parallel_recognition","text":"<pre><code>run_parallel_recognition()\n</code></pre> <p>Run face recognition on all images using parallel processing.</p> Source code in <code>src\\recognizer.py</code> <pre><code>def run_parallel_recognition(self):\n    \"\"\"\n    Run face recognition on all images using parallel processing.\n    \"\"\"\n    total_start = time.time()\n\n    # load known face\n    self._load_known_face()\n\n    # get all image files\n    filenames = self._get_image_files()\n    total_images = len(filenames)\n\n    if total_images == 0:\n        print(\"no images found to process\")\n        return [], 0\n\n    # determine optimal configuration\n    print(\"\\nconfiguring worker allocation...\")\n    print(f\"   available cpu cores: {cpu_count()}\")\n    print(f\"   dataset size: {total_images} images\")\n    print()\n\n    num_processes, chunksize = self._determine_optimal_processes(total_images)\n\n    # show workload distribution\n    self._calculate_workload_distribution(total_images, num_processes, chunksize)\n\n    # create worker function with fixed parameters\n    process_func = partial(\n        _worker_process_image,\n        known_encoding=self.known_encoding,\n        folder_path=self.image_folder,\n        output_folder=self.output_folder\n    )\n\n    # execute parallel processing\n    print(f\"\\nprocessing {total_images} images with {num_processes} workers...\")\n    print(f\"   (dynamic load balancing with chunk size: {chunksize})\")\n\n    processing_start = time.time()\n\n    with Pool(processes=num_processes) as pool:\n        results = pool.map(process_func, filenames, chunksize=chunksize)\n\n    self.performance_stats['processing_time'] = time.time() - processing_start\n\n    # filter and collect results\n    print(\"\\ncollecting results...\")\n\n    matched_results = [r for r in results if r is not None]\n    matched_files = [filename for filename, _ in matched_results]\n\n    if matched_results:\n        processing_times = [proc_time for _, proc_time in matched_results]\n        avg_match_time = sum(processing_times) / len(processing_times)\n        print(f\"   average time per matched image: {avg_match_time:.3f}s\")\n\n    self.performance_stats['total_time'] = time.time() - total_start\n    self.performance_stats['num_workers'] = num_processes\n    self.performance_stats['chunksize'] = chunksize\n\n    return matched_files, total_images\n</code></pre>"},{"location":"serial/","title":"Serial Implementation","text":"<p>The serial implementation processes images one at a time using a single CPU core. It is used to provide a baseline for evaluating the benefits of parallel execution.</p> <p>Two serial approaches exist in this project:</p> <ul> <li>Standalone serial program: <code>task1_4_serial.py</code> </li> <li>Integrated serial comparison inside <code>task_1_4_main.py</code> </li> </ul>"},{"location":"serial/#purpose","title":"Purpose","text":"<ul> <li>Establish baseline execution time  </li> <li>Validate recognition correctness  </li> <li>Calculate speedup and efficiency  </li> </ul>"},{"location":"serial/#api-reference-integrated-serial-mode","title":"API Reference (integrated serial mode)","text":""},{"location":"serial/#task_1_4_main.run_serial_comparison","title":"run_serial_comparison","text":"<pre><code>run_serial_comparison(known_image_path, image_folder_path, output_folder_path)\n</code></pre> <p>run serial version for performance comparison.</p> <p>Returns:</p> Type Description <p>serial execution time in seconds</p> Source code in <code>task_1_4_main.py</code> <pre><code>def run_serial_comparison(known_image_path, image_folder_path, \n                         output_folder_path):\n    \"\"\"\n    run serial version for performance comparison.\n\n    returns:\n        serial execution time in seconds\n    \"\"\"\n    import face_recognition\n\n    print_section(\"serial processing (for comparison)\")\n\n    start_time = time.time()\n\n    # load known face\n    print(\"loading known face...\")\n    known_image = face_recognition.load_image_file(known_image_path)\n    known_encoding = face_recognition.face_encodings(known_image)[0]\n\n    # get all images\n    print(\"scanning images...\")\n    filenames = [f.name for f in os.scandir(image_folder_path) if f.is_file()]\n    total_images = len(filenames)\n\n    print(f\"processing {total_images} images serially (one at a time)...\")\n    matches = 0\n\n    # process each image one by one (serial)\n    for i, filename in enumerate(filenames, 1):\n        try:\n            image_path = os.path.join(image_folder_path, filename)\n            unknown_image = face_recognition.load_image_file(image_path)\n            unknown_encodings = face_recognition.face_encodings(unknown_image)\n\n            for unknown_encoding in unknown_encodings:\n                result = face_recognition.compare_faces(\n                    [known_encoding], unknown_encoding, tolerance=0.6\n                )\n                if result[0]:\n                    matches += 1\n                    print(f\"  [{i}/{total_images}] match: {filename}\")\n                    break\n        except:\n            pass\n\n    serial_time = time.time() - start_time\n\n    print(f\"\\nserial processing complete\")\n    print(f\"  total time: {serial_time:.2f}s\")\n    print(f\"  matches found: {matches}\")\n\n    return serial_time\n</code></pre>"},{"location":"utils/","title":"Utility Module","text":"<p>This module provides shared helper functions used by the recognition system.</p>"},{"location":"utils/#responsibilities","title":"Responsibilities","text":"<ul> <li>Support functions for recognizer logic  </li> <li>Reusable utilities  </li> <li>Internal processing helpers  </li> </ul>"},{"location":"utils/#api-reference","title":"API Reference","text":""},{"location":"utils/#src.utils","title":"utils","text":""},{"location":"utils/#src.utils.calculate_speedup","title":"calculate_speedup","text":"<pre><code>calculate_speedup(serial_time, parallel_time, num_processes)\n</code></pre> <p>Compute speedup, efficiency, and overhead.</p> Source code in <code>src\\utils.py</code> <pre><code>def calculate_speedup(serial_time, parallel_time, num_processes):\n    \"\"\"\n    Compute speedup, efficiency, and overhead.\n    \"\"\"\n    if parallel_time == 0:\n        return {\n            'speedup': 0,\n            'efficiency': 0,\n            'overhead': 0,\n            'time_saved': 0\n        }\n\n    speedup = serial_time / parallel_time\n    efficiency = (speedup / num_processes) * 100\n    overhead = (parallel_time * num_processes) - serial_time\n    time_saved = serial_time - parallel_time\n\n    return {\n        'speedup': speedup,\n        'efficiency': efficiency,\n        'overhead': overhead,\n        'time_saved': time_saved\n    }\n</code></pre>"},{"location":"utils/#src.utils.format_time","title":"format_time","text":"<pre><code>format_time(seconds)\n</code></pre> <p>Convert seconds into a readable time string.</p> Source code in <code>src\\utils.py</code> <pre><code>def format_time(seconds):\n    \"\"\"\n    Convert seconds into a readable time string.\n    \"\"\"\n    if seconds &lt; 60:\n        return f\"{seconds:.2f}s\"\n    elif seconds &lt; 3600:\n        minutes = int(seconds // 60)\n        secs = seconds % 60\n        return f\"{minutes}m {secs:.2f}s\"\n    else:\n        hours = int(seconds // 3600)\n        remainder = seconds % 3600\n        minutes = int(remainder // 60)\n        secs = remainder % 60\n        return f\"{hours}h {minutes}m {secs:.2f}s\"\n</code></pre>"},{"location":"utils/#src.utils.get_image_files","title":"get_image_files","text":"<pre><code>get_image_files(folder_path)\n</code></pre> <p>Return valid image filenames from a folder.</p> Source code in <code>src\\utils.py</code> <pre><code>def get_image_files(folder_path):\n    \"\"\"\n    Return valid image filenames from a folder.\n    \"\"\"\n    if not os.path.exists(folder_path):\n        return []\n\n    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff'}\n\n    image_files = []\n    for entry in os.scandir(folder_path):\n        if entry.is_file():\n            _, ext = os.path.splitext(entry.name.lower())\n            if ext in valid_extensions:\n                image_files.append(entry.name)\n\n    return image_files\n</code></pre>"},{"location":"utils/#src.utils.save_found_image","title":"save_found_image","text":"<pre><code>save_found_image(unknown_image, filename, output_folder, matched_face_location=None)\n</code></pre> <p>Draw bounding box only on the matched face and save the image.</p> <p>Parameters:</p> Name Type Description Default <code>unknown_image</code> <p>The image in RGB format</p> required <code>filename</code> <p>Original filename</p> required <code>output_folder</code> <p>Where to save the result</p> required <code>matched_face_location</code> <p>(top, right, bottom, left) of the matched face only</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if saved successfully</p> Explanation <p>The function draws a bounding box around the matched face in the image</p> Source code in <code>src\\utils.py</code> <pre><code>def save_found_image(unknown_image, filename, output_folder, \n                     matched_face_location = None):\n    \"\"\"\n    Draw bounding box only on the matched face and save the image.\n\n    Args:\n        unknown_image: The image in RGB format\n        filename: Original filename\n        output_folder: Where to save the result\n        matched_face_location: (top, right, bottom, left) of the matched face only\n\n    Returns:\n        bool: True if saved successfully\n\n    Explanation:\n        The function draws a bounding box around the matched face in the image\n    \"\"\"\n    try:\n        # ensure output directory exists\n        if not os.path.exists(output_folder):\n            os.makedirs(output_folder)\n\n        # if no specific face location provided, draw on all faces (fallback)\n        if matched_face_location is None:\n            face_locations = face_recognition.face_locations(unknown_image)\n            if not face_locations:\n                print(f\"   no faces detected in {filename} during save\")\n                return False\n        else:\n            # use only the matched face location\n            face_locations = [matched_face_location]\n\n        # draw bounding box only on the matched face(s)\n        for top, right, bottom, left in face_locations:\n            # draw green rectangle with thickness 3 for better visibility\n            cv2.rectangle(\n                unknown_image, \n                (left, top),      # top-left corner\n                (right, bottom),  # bottom-right corner\n                (0, 255, 0),      # green color in rgb\n                3                 # line thickness\n            )\n\n            # add label \"match\" above the bounding box\n            label = \"MATCH\"\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            font_scale = 0.8\n            font_thickness = 2\n\n            # get text size for background rectangle\n            (text_width, text_height), baseline = cv2.getTextSize(\n                label, font, font_scale, font_thickness\n            )\n\n            # draw background rectangle for text\n            cv2.rectangle(\n                unknown_image,\n                (left, top - text_height - 10),\n                (left + text_width + 10, top),\n                (0, 255, 0),  # green background\n                -1  # filled rectangle\n            )\n\n            # draw text\n            cv2.putText(\n                unknown_image,\n                label,\n                (left + 5, top - 5),\n                font,\n                font_scale,\n                (0, 0, 0),  # black text\n                font_thickness\n            )\n\n        # convert rgb to bgr for opencv\n        image_bgr = cv2.cvtColor(unknown_image, cv2.COLOR_RGB2BGR)\n\n        # create unique filename and save\n        base_name = os.path.splitext(os.path.basename(filename))[0]\n        timestamp = int(time.time())\n        output_filename = f\"detected_{base_name}_{timestamp}.jpg\"\n        output_path = os.path.join(output_folder, output_filename)\n\n        # save image\n        success = cv2.imwrite(output_path, image_bgr)\n\n        if success:\n            print(f\"   saved: {output_filename}\")\n\n        return success\n\n    except Exception as e:\n        print(f\"  error saving {filename}: {str(e)}\")\n        return False\n</code></pre>"},{"location":"utils/#src.utils.validate_image_file","title":"validate_image_file","text":"<pre><code>validate_image_file(filepath)\n</code></pre> <p>Check whether a file is a valid image.</p> Source code in <code>src\\utils.py</code> <pre><code>def validate_image_file(filepath):\n    \"\"\"\n    Check whether a file is a valid image.\n    \"\"\"\n    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff'}\n\n    # check extension\n    _, ext = os.path.splitext(filepath.lower())\n    if ext not in valid_extensions:\n        return False\n\n    # check if file exists and is readable\n    if not os.path.isfile(filepath):\n        return False\n\n    # check file size (not empty)\n    if os.path.getsize(filepath) == 0:\n        return False\n\n    return True\n</code></pre>"}]}